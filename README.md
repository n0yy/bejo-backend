# ğŸ¤– BEJO Backend - RAG Service

Backend for a Retrieval-Augmented Generation (RAG) service built with FastAPI, LangGraph, Ollama, and Qdrant for an intelligent knowledge base system.

## ğŸš€ Key Features

- **Multi-level Knowledge Base**: Supports 4 levels of knowledge categorization
- **Chat with Memory**: Thread-based conversations with memory
- **Document Upload**: Supports PDF, DOCX, PPTX, HTML, TXT, CSV, and image files
- **Vector Search**: Semantic search powered by Qdrant
- **Health Check**: System health monitoring
- **RESTful API**: Complete and user-friendly endpoints

## ğŸ› ï¸ Tech Stack

- **FastAPI**: Modern Python web framework
- **LangChain/LangGraph**: LLM and RAG workflow orchestration
- **Ollama**: Local LLM inference
- **Qdrant**: Vector database for similarity search
- **Docker**: Containerization and deployment
- **UV**: Fast Python package manager

## ğŸ“‹ Requirements

- Docker & Docker Compose
- Python 3.11+ (for development)
- Running Ollama server
- Running Qdrant server

## ğŸ”§ Setup and Installation

### 1. Clone the Repository

```bash
git clone <repository-url>
cd bejo-backend
```

### 2. Configure Environment

Create a `.env` file based on `.env.example`:

```env
QDRANT_HOST=localhost
QDRANT_PORT=6333
OLLAMA_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_LLM_MODEL=qwen2.5:7b
```

### 3. Run with Docker

#### CPU Mode:

```bash
docker-compose --profile cpu up -d --build
```

#### GPU Mode (if available):

```bash
docker-compose --profile gpu up -d --build
```

### 4. Verify Installation

```bash
# Check container status
docker-compose ps

# Test health check
curl http://localhost:8000/health
```

## ğŸ“š API Endpoints

### ğŸ¥ Health Check

```http
GET /health
```

Check the health of the system (Qdrant, Ollama embeddings).

**Response:**

```json
{
  "status": "healthy",
  "qdrant": "connected",
  "ollama_embeddings": "working",
  "embedding_size": 768
}
```

### ğŸ“„ Upload Document

```http
POST /upload?category={1-4}&embed=true
Content-Type: multipart/form-data

file: <file>
```

**Parameters:**

- `category`: Knowledge category level (1, 2, 3, or 4)
- `embed`: Whether the document should be embedded (default: true)

**Supported File Types:**

- ğŸ“„ PDF, DOCX, PPTX
- ğŸŒ HTML, TXT, CSV
- ğŸ–¼ï¸ PNG, JPG, JPEG, GIF, WebP, TIFF

**Response:**

```json
{
  "message": "Document uploaded and embedded successfully",
  "filename": "document.pdf",
  "document_id": "uuid-string",
  "chunks_created": 15
}
```

### ğŸ’¬ Chat

```http
POST /chat/{thread_id}
Content-Type: application/json

{
  "question": "Your question",
  "category": "1"
}
```

**Response:**

```json
{
  "answer": "AI's answer with emoji ğŸ‰",
  "thread_id": "thread-123",
  "sources": [
    {
      "filename": "document.pdf",
      "document_id": "uuid-string",
      "file_path": "/path/to/file"
    }
  ]
}
```

### ğŸ“‹ Chat History

```http
GET /chat/history/{thread_id}
```

**Response:**

```json
{
  "thread_id": "thread-123",
  "messages": [
    {
      "type": "user",
      "content": "User's question",
      "timestamp": "2024-01-01T00:00:00Z"
    },
    {
      "type": "assistant",
      "content": "AI's answer",
      "timestamp": "2024-01-01T00:00:01Z"
    }
  ],
  "total_messages": 2
}
```

### ğŸ—‚ï¸ Vector Store Management

#### View All Data

```http
GET /vectorstore/bejo-knowledge-level-{1-4}
```

#### Delete Data

```http
DELETE /vectorstore/bejo-knowledge-level-{1-4}?id={point_id}
```

#### Update Data

```http
PUT /vectorstore/bejo-knowledge-level-{1-4}?id={point_id}
Content-Type: application/json

{
  "page_content": "Updated content",
  "file_path": "/path/to/file",
  "uploaded_at": "2024-01-01T00:00:00Z"
}
```
## ğŸ§  How RAG Works

1. **ğŸ“„ Document Processing**: Documents are processed using Docling for text extraction
2. **âœ‚ï¸ Text Splitting**: Text is split into chunks using `MarkdownHeaderTextSplitter`
3. **ğŸŒ¤ï¸ Embedding**: Each chunk is converted into vectors using Ollama embeddings
4. **ğŸ—‚ï¸ Storage**: Vectors are stored in Qdrant under the appropriate category
5. **ğŸ” Retrieval**: On user query, relevant chunks are retrieved
6. **ğŸ¤– Generation**: LLM generates an answer based on the retrieved context

## ğŸ³ Docker Commands

```bash
# Build and start
docker-compose --profile cpu up -d --build

# Stop services
docker-compose down

# Restart services
docker-compose restart

# View logs
docker-compose logs -f bejo-backend

# Rebuild without cache
docker-compose build --no-cache

# Cleanup (âš ï¸ caution!)
docker system prune -af --volumes
```

## ğŸ“ Project Structure

```
bejo-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/           # API endpoints
â”‚   â”œâ”€â”€ core/          # Core components (LLM, embeddings, etc.)
â”‚   â”œâ”€â”€ models/        # Pydantic models
â”‚   â””â”€â”€ services/      # Business logic
â”œâ”€â”€ uploads/           # Directory for uploaded files
â”œâ”€â”€ Dockerfile         # Container configuration
â”œâ”€â”€ docker-compose.yml # Multi-container setup
â””â”€â”€ pyproject.toml     # Dependencies
```

## ğŸ”§ Development

### Local Development Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Run development server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Environment Variables

```env
# Qdrant Configuration
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_LLM_MODEL=qwen2.5:7b
```

## ğŸ“Š Monitoring and Troubleshooting

### Health Check

```bash
curl http://localhost:8000/health
```

### Log Monitoring

```bash
# Realtime logs
docker-compose logs -f bejo-backend

# Specific container logs
docker logs <container_id>
```

### Debug Container

```bash
# Enter container
docker exec -it bejo-backend bash

# Check processes
docker exec -it bejo-backend ps aux
```
